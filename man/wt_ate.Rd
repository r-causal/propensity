% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/weights.R
\name{wt_ate}
\alias{wt_ate}
\alias{wt_ate.data.frame}
\alias{wt_att}
\alias{wt_att.data.frame}
\alias{wt_atu}
\alias{wt_atu.data.frame}
\alias{wt_atm}
\alias{wt_atm.data.frame}
\alias{wt_ato}
\alias{wt_ato.data.frame}
\alias{wt_entropy}
\alias{wt_entropy.data.frame}
\alias{wt_atc}
\alias{wt_cens}
\alias{wt_cens.data.frame}
\title{Calculate Propensity Score Weights for Causal Inference}
\usage{
wt_ate(
  .propensity,
  .exposure,
  .sigma = NULL,
  exposure_type = c("auto", "binary", "categorical", "continuous"),
  .treated = NULL,
  .untreated = NULL,
  stabilize = FALSE,
  stabilization_score = NULL,
  ...
)

\method{wt_ate}{data.frame}(
  .propensity,
  .exposure,
  .sigma = NULL,
  exposure_type = c("auto", "binary", "categorical", "continuous"),
  .treated = NULL,
  .untreated = NULL,
  stabilize = FALSE,
  stabilization_score = NULL,
  ...,
  .propensity_col = NULL
)

wt_att(
  .propensity,
  .exposure,
  exposure_type = c("auto", "binary", "categorical"),
  .treated = NULL,
  .untreated = NULL,
  ...,
  .focal_level = NULL
)

\method{wt_att}{data.frame}(
  .propensity,
  .exposure,
  exposure_type = c("auto", "binary", "categorical"),
  .treated = NULL,
  .untreated = NULL,
  ...,
  .propensity_col = NULL,
  .focal_level = NULL
)

wt_atu(
  .propensity,
  .exposure,
  exposure_type = c("auto", "binary", "categorical"),
  .treated = NULL,
  .untreated = NULL,
  ...,
  .focal_level = NULL
)

\method{wt_atu}{data.frame}(
  .propensity,
  .exposure,
  exposure_type = c("auto", "binary", "categorical"),
  .treated = NULL,
  .untreated = NULL,
  ...,
  .propensity_col = NULL,
  .focal_level = NULL
)

wt_atm(
  .propensity,
  .exposure,
  exposure_type = c("auto", "binary", "categorical"),
  .treated = NULL,
  .untreated = NULL,
  ...
)

\method{wt_atm}{data.frame}(
  .propensity,
  .exposure,
  exposure_type = c("auto", "binary", "categorical"),
  .treated = NULL,
  .untreated = NULL,
  ...,
  .propensity_col = NULL
)

wt_ato(
  .propensity,
  .exposure,
  exposure_type = c("auto", "binary", "categorical"),
  .treated = NULL,
  .untreated = NULL,
  ...
)

\method{wt_ato}{data.frame}(
  .propensity,
  .exposure,
  exposure_type = c("auto", "binary", "categorical"),
  .treated = NULL,
  .untreated = NULL,
  ...,
  .propensity_col = NULL
)

wt_entropy(
  .propensity,
  .exposure,
  exposure_type = c("auto", "binary", "categorical"),
  .treated = NULL,
  .untreated = NULL,
  ...
)

\method{wt_entropy}{data.frame}(
  .propensity,
  .exposure,
  exposure_type = c("auto", "binary", "categorical"),
  .treated = NULL,
  .untreated = NULL,
  ...,
  .propensity_col = NULL
)

wt_atc(
  .propensity,
  .exposure,
  exposure_type = c("auto", "binary", "categorical"),
  .treated = NULL,
  .untreated = NULL,
  ...,
  .focal_level = NULL
)

wt_cens(
  .propensity,
  .exposure,
  .sigma = NULL,
  exposure_type = c("auto", "binary", "categorical", "continuous"),
  .treated = NULL,
  .untreated = NULL,
  stabilize = FALSE,
  stabilization_score = NULL,
  ...
)

\method{wt_cens}{data.frame}(
  .propensity,
  .exposure,
  .sigma = NULL,
  exposure_type = c("auto", "binary", "categorical", "continuous"),
  .treated = NULL,
  .untreated = NULL,
  stabilize = FALSE,
  stabilization_score = NULL,
  ...,
  .propensity_col = NULL
)
}
\arguments{
\item{.propensity}{Either a numeric vector of predicted probabilities, a
\code{data.frame} where each column corresponds to a level of the exposure,
or a fitted GLM object. For data frames, the second column is used by
default for binary exposures unless specified otherwise with
\code{.propensity_col}. For GLM objects, fitted values are extracted
automatically.}

\item{.exposure}{The exposure variable. For binary exposures, a vector of 0s
and 1s; for continuous exposures, a numeric vector. When \code{.propensity} is
a GLM object, this argument is optional and will be extracted from the
model if not provided.}

\item{.sigma}{For continuous exposures, a numeric vector of standard errors
used with \code{dnorm()}. For example, this can be derived from the influence
measures of a model (e.g., \code{influence(model)$sigma}).}

\item{exposure_type}{Character string specifying the type of exposure.
Options are \code{"auto"}, \code{"binary"}, \code{"categorical"}, and \code{"continuous"}.
Defaults to \code{"auto"}, which detects the type automatically.}

\item{.treated}{The value representing the treatment group. If not provided,
it is automatically detected.}

\item{.untreated}{The value representing the control group. If not provided,
it is automatically detected.}

\item{stabilize}{Logical indicating whether to stabilize the weights. For ATE
weights, stabilization multiplies the weight by either the mean of
\code{.exposure} or the supplied \code{stabilization_score}. Note: stabilization is only
supported for ATE and continuous exposures.}

\item{stabilization_score}{Optional numeric value for stabilizing the weights
(e.g., a predicted value from a regression model without predictors). Only
used when \code{stabilize} is \code{TRUE}.}

\item{...}{Reserved for future expansion. Not currently used.}

\item{.propensity_col}{With a binary exposure, when \code{.propensity} is a data frame, specifies which
column to use for propensity scores. Can be a column name (quoted or
unquoted) or a numeric index. Defaults to the second column if available,
otherwise the first. For categorical exposures, the entire data frame is
used as a matrix of propensity scores.}

\item{.focal_level}{For categorical exposures with ATT or ATU estimands, specifies
the focal category. Must be one of the levels of the exposure variable.
Required for \code{wt_att()} and \code{wt_atu()} with categorical exposures.}
}
\value{
A \code{psw} object (a numeric vector) with additional attributes:
\itemize{
\item \strong{estimand}: A description of the estimand (e.g., "ate", "att").
\item \strong{stabilized}: A logical flag indicating if stabilization was applied.
\item \strong{trimmed}: A logical flag indicating if the weights are based on trimmed propensity scores.
\item \strong{truncated}: A logical flag indicating if the weights are based on truncated propensity scores.
}
}
\description{
This family of functions computes propensity score weights for
various causal estimands:
\itemize{
\item \strong{ATE} (Average Treatment Effect)
\item \strong{ATT} (Average Treatment Effect on the Treated)
\item \strong{ATU} (Average Treatment Effect on the Untreated, sometimes called
the \strong{ATC}, where the "C" stands for "control"). \code{wt_atc()} is provided
as an alias for \code{wt_atu()}
\item \strong{ATM} (Average Treatment Effect for the Evenly Matchable)
\item \strong{ATO} (Average Treatment Effect for the Overlap population)
\item \strong{Entropy} (Average Treatment Effect for the Entropy-weighted population)
\item \strong{Censoring weights} can be calculated using \code{wt_cens()}, which uses
the same formula as ATE weights but with estimand "uncensored". These
are useful for handling censoring in survival analysis

The propensity score can be provided as a numeric vector of predicted
probabilities, as a \code{data.frame} where each column represents the
predicted probability for a level of the exposure, or as a fitted
GLM object. They can also be propensity score objects created by
\code{\link[=ps_trim]{ps_trim()}}, \code{\link[=ps_refit]{ps_refit()}}, or \code{\link[=ps_trunc]{ps_trunc()}}

The returned weights are encapsulated in a \code{psw} object, which is a numeric
vector with additional attributes that record the estimand, and whether the
weights have been stabilized, trimmed, or truncated.
}
}
\details{
\subsection{Theoretical Background}{

Propensity score weighting is a method for estimating causal effects by
creating a pseudo-population where the exposure is independent of measured
confounders. The propensity score, \eqn{e(X)}, is the probability of receiving
treatment given observed covariates \eqn{X}. By weighting observations inversely
proportional to their propensity scores, we can balance the distribution of
covariates between treatment groups. Other weights allow for different target populations.
}

\subsection{Mathematical Formulas}{
\subsection{Binary Exposures}{

For binary treatments (\eqn{A = 0} or \eqn{1}), the weights are:
\itemize{
\item \strong{ATE}: \eqn{w = \frac{A}{e(X)} + \frac{1-A}{1-e(X)}}
\item \strong{ATT}: \eqn{w = A + \frac{(1-A) \cdot e(X)}{1-e(X)}}
\item \strong{ATU}: \eqn{w = \frac{A \cdot (1-e(X))}{e(X)} + (1-A)}
\item \strong{ATM}: \eqn{w = \frac{\min(e(X), 1-e(X))}{A \cdot e(X) + (1-A) \cdot (1-e(X))}}
\item \strong{ATO}: \eqn{w = A \cdot (1-e(X)) + (1-A) \cdot e(X)}
\item \strong{Entropy}: \eqn{w = \frac{h(e(X))}{A \cdot e(X) + (1-A) \cdot (1-e(X))}}, where \eqn{h(e) = -[e \cdot \log(e) + (1-e) \cdot \log(1-e)]}
}
}

\subsection{Continuous Exposures}{

For continuous treatments, weights use the density ratio:
\eqn{w = \frac{f_A(A)}{f_{A|X}(A|X)}}, where \eqn{f_A} is the marginal density of \eqn{A}
and \eqn{f_{A|X}} is the conditional density given \eqn{X}.
}

\subsection{Categorical Exposures}{

For categorical treatments with \eqn{K} levels, weights use a tilting function approach:
\eqn{w_i = \frac{h(e_i)}{e_{i,Z_i}}}, where \eqn{e_{i,Z_i}} is the propensity score for unit \eqn{i}'s
observed treatment level, and \eqn{h(e_i)} is a tilting function that depends on the estimand:
\itemize{
\item \strong{ATE}: \eqn{h(e) = 1}
\item \strong{ATT}: \eqn{h(e) = e_{focal}} (propensity score for the focal category)
\item \strong{ATU}: \eqn{h(e) = 1 - e_{focal}} (complement of focal category propensity)
\item \strong{ATM}: \eqn{h(e) = \min(e_1, ..., e_K)}
\item \strong{ATO}: \eqn{h(e) = 1 / \sum_k(1/e_k)} (reciprocal of harmonic mean denominator)
\item \strong{Entropy}: \eqn{h(e) = -\sum_k[e_k \cdot \log(e_k)]} (entropy of propensity scores)
}
}

}

\subsection{Exposure Types}{

The functions support different types of exposures:
\itemize{
\item \strong{\code{binary}}: For dichotomous treatments (e.g. 0/1).
\item \strong{\code{continuous}}: For numeric exposures. Here, weights are calculated via the normal density using
\code{dnorm()}.
\item \strong{\code{categorical}}: For exposures with more than 2 categories. Requires \code{.propensity} to be a
matrix or data frame with columns representing propensity scores for each category.
\item \strong{\code{auto}}: Automatically detects the exposure type based on \code{.exposure}.
}
}

\subsection{Stabilization}{

For ATE weights, stabilization can improve the performance of the estimator
by reducing variance. When \code{stabilize} is \code{TRUE} and no
\code{stabilization_score} is provided, the weights are multiplied by the mean
of \code{.exposure}. Alternatively, if a \code{stabilization_score} is provided, it
is used as the multiplier. Stabilized weights have the form:
\eqn{w_s = f_A(A) \times w}, where \eqn{f_A(A)} is the marginal probability or density.
}

\subsection{Weight Properties and Diagnostics}{

Extreme weights can indicate:
\itemize{
\item Positivity violations (near 0 or 1 propensity scores)
\item Poor model specification
\item Lack of overlap between treatment groups
}

See the halfmoon package for tools to diagnose and visualize weights.

You can address extreme weights in several ways. The first is to modify the target population:
use trimming, truncation, or alternative estimands (ATM, ATO, entropy).
Another technique that can help is stabilization, which reduces variance of the weights.
}

\subsection{Trimmed and Truncated Weights}{

In addition to the standard weight functions, versions exist for trimmed
and truncated propensity score weights created by \code{\link[=ps_trim]{ps_trim()}},
\code{\link[=ps_trunc]{ps_trunc()}}, and \code{\link[=ps_refit]{ps_refit()}}. These variants calculate the weights using
modified propensity scores (trimmed or truncated) and update the estimand
attribute accordingly.
}
}
\examples{
## Basic Usage with Binary Exposures

# Simulate a simple dataset
set.seed(123)
n <- 100
propensity_scores <- runif(n, 0.1, 0.9)
treatment <- rbinom(n, 1, propensity_scores)

# Calculate different weight types
weights_ate <- wt_ate(propensity_scores, treatment)
weights_att <- wt_att(propensity_scores, treatment)
weights_atu <- wt_atu(propensity_scores, treatment)
weights_atm <- wt_atm(propensity_scores, treatment)
weights_ato <- wt_ato(propensity_scores, treatment)
weights_entropy <- wt_entropy(propensity_scores, treatment)

# Compare weight distributions
summary(weights_ate)
summary(weights_ato)  # Often more stable than ATE

## Stabilized Weights

# Stabilization reduces variance
weights_ate_stab <- wt_ate(propensity_scores, treatment, stabilize = TRUE)

## Handling Extreme Propensity Scores

# Create data with positivity violations
ps_extreme <- c(0.01, 0.02, 0.98, 0.99, rep(0.5, 4))
trt_extreme <- c(0, 0, 1, 1, 0, 1, 0, 1)

# Standard ATE weights can be extreme
wt_extreme <- wt_ate(ps_extreme, trt_extreme)
# Very large!
max(wt_extreme)

# ATO weights are bounded
wt_extreme_ato <- wt_ato(ps_extreme, trt_extreme)
# Much more reasonable
max(wt_extreme_ato)
# but they target a different population
estimand(wt_extreme_ato) # "ato"

## Working with Data Frames

# Example with custom data frame
ps_df <- data.frame(
  control = c(0.9, 0.7, 0.3, 0.1),
  treated = c(0.1, 0.3, 0.7, 0.9)
)
exposure <- c(0, 0, 1, 1)

# Uses second column by default (treated probabilities)
wt_ate(ps_df, exposure)

# Explicitly specify column by name
wt_ate(ps_df, exposure, .propensity_col = "treated")

# Or by position
wt_ate(ps_df, exposure, .propensity_col = 2)

## Working with GLM Objects

# Fit a propensity score model
set.seed(123)
n <- 100
x1 <- rnorm(n)
x2 <- rnorm(n)
treatment <- rbinom(n, 1, plogis(0.5 * x1 + 0.3 * x2))

ps_model <- glm(treatment ~ x1 + x2, family = binomial)

# Use GLM directly for weight calculation
weights_from_glm <- wt_ate(ps_model, treatment)

# Or omit the exposure argument (it will be extracted from the GLM)
weights_from_glm_auto <- wt_ate(ps_model)

}
\references{
For detailed guidance on causal inference in R, see \href{https://www.r-causal.org/}{\emph{Causal Inference in R}}
by Malcolm Barrett, Lucy D'Agostino McGowan, and Travis Gerke.
\subsection{Foundational Papers}{

Rosenbaum, P. R., & Rubin, D. B. (1983). The central role of the propensity
score in observational studies for causal effects. \emph{Biometrika}, 70(1), 41-55.
}

\subsection{Estimand-Specific Methods}{

Li, L., & Greene, T. (2013). A weighting analogue to pair matching in
propensity score analysis. \emph{The International Journal of Biostatistics}, 9(2),
215-234. (ATM weights)

Li, F., Morgan, K. L., & Zaslavsky, A. M. (2018). Balancing covariates via
propensity score weighting. \emph{Journal of the American Statistical Association},
113(521), 390-400. (ATO weights)

Zhou, Y., Matsouaka, R. A., & Thomas, L. (2020). Propensity score weighting
under limited overlap and model misspecification. \emph{Statistical Methods in
Medical Research}, 29(12), 3721-3756. (Entropy weights)
}

\subsection{Continuous Exposures}{

Hirano, K., & Imbens, G. W. (2004). The propensity score with continuous
treatments. \emph{Applied Bayesian Modeling and Causal Inference from
Incomplete-Data Perspectives}, 226164, 73-84.
}

\subsection{Practical Guidance}{

Austin, P. C., & Stuart, E. A. (2015). Moving towards best practice when
using inverse probability of treatment weighting (IPTW) using the propensity
score to estimate causal treatment effects in observational studies.
\emph{Statistics in Medicine}, 34(28), 3661-3679.
}
}
\seealso{
\itemize{
\item \code{\link[=psw]{psw()}} for details on the structure of the returned weight objects.
\item \code{\link[=ps_trim]{ps_trim()}}, \code{\link[=ps_trunc]{ps_trunc()}}, and \code{\link[=ps_refit]{ps_refit()}} for handling extreme weights.
\item \code{\link[=ps_calibrate]{ps_calibrate()}} for calibrating weights.
}
}
